batch_size：在训练模型的时候，我们不会一次性将所有数据输入模型（计算量太大计算成本高），而是将数据集分成若干批次（batch），逐批输入模型。（注意batch_size需要）
（eg：batch_size=8 表示：每批包含 8 个样本（如 8 张图像、8 条文本、8 个点云等），模型会基于这 8 个样本的平均损失计算梯度，并更新一次参数。）
为什么会有batch？因为GPU/TPU等硬件的核心优势就是“并行运算”--同时处理多个样本的计算
batch的优点是？

1. 提升计算效率
2. 平滑梯度波动：若batch_size=1，那么梯度仅由1个样本决定，波动极大（可能受噪声样本影响），但是如果batch_size=8（假设），梯度就会是8个样本的平均，能“平滑”单个样本的噪声，梯度方向更稳定，训练曲线更平缓，模型更容易收敛。

模型中参数的选择可从githup源码中get参考。