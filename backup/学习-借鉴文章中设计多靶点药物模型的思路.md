文章1思路：
使用预训练的化学模型（LSTM），将两个靶点的已知配体合并成一个训练集进行模型微调。微调后，模型生成的分子会倾向于同时具有两个靶点的药效团特征，从而可能成为双靶点。

微调原理（fine-tuning）：通过用目标数据（双靶点配体的合并集）继续训练一个预训练模型【微调】，轻微调整其庞大的内部参数，从而偏置（bias）模型的输出概率分布，使其生成的新分子更可能具备训练数据所代表的化学空间区域的特性（即同时包含两个靶点的药效团特征），而不是机械地复制训练数据。

第一步：预训练化学语言模型（CLM），先掌握“smile语法与通用化学规律”。预训练目标：让模型能 “生成合法 SMILES”—— 输入部分 SMILES 字符，模型能按化学规则补全剩余部分，且生成的分子结构符合基本化学规律（如无无效原子连接、电荷中性）。CLM核心工作机制：从起始字符预测下一个字符的概率。（CLM的本质是LSTM的序列生成模型）
第二步：双靶点定向微调，合并两个靶点的配体作为训练集
文章针对每一个目标靶点对单独构建微调集并训练独立模型，并非将所有靶点对的配体混合成一个训练集。构建微调集：
1. 筛选高质量单靶点配体，从BindingDB数据库中筛选每个靶点的高活性配体（IC₅₀/EC₅₀≤1μM），通过 k-means 聚类（基于 Morgan 指纹）确保配体的化学多样性（避免同类结构重复），最终每个靶点仅保留 5-9 个 “结构多样且活性强” 的配体作为 “微调模板”（如 FXR 选 5 个、sEH 选 6 个），且这些模板均无已知双靶点活性（避免模型直接复制已有双靶点分子）。
2. 构建单靶点对的 “合并微调集”，针对一个靶点对（如 FXR/sEH），将 FXR 的 5 个模板配体与 sEH 的 6 个模板配体合并为一个微调集（共 11 个分子），不混合其他靶点对的配体（如不将 FXR/sEH 与 PPARδ/sEH 的配体合并）。每个靶点对对应一个独立的合并微调集，后续训练独立的 CLM 模型。
3. 优化微调策略 —— 仅 “Pooled 微调” 能实现双靶点平衡。文章测试了三种微调策略（、），最终选择 “Pooled 微调”（将两个靶点的配体混合后同时输入模型），核心原因是：
「Sequential 微调」：先训靶点 A，再训靶点 B—— 模型会先偏向 A，但训 B 时会快速遗忘 A 的特征，无法平衡；
「Alternating 微调」：每轮交替输入 A、B 的配体 —— 模型在 A、B 特征间反复切换，无法稳定学习双靶点共性；
「Pooled 微调」：将 A、B 配体混合成一个集合输入 —— 模型能同时学习两个靶点的配体特征，生成的分子对 A、B 的结构相似性更平衡（如图 3a，PPARδ/sEH 靶点对的 Pooled 微调后，生成分子与两个靶点模板的 Tanimoto 相似度均稳定在 0.3-0.4，而其他策略仅偏向单一靶点）。
第三步：分子生成与筛选。分子生成：通过两种采样方式从微调后的CLM中生成分子。「Beam 搜索」：启发式搜索（beam 宽度 50），优先生成模型预测概率高的分子，用于监控微调效果（如判断哪一轮 epoch 的模型生成效果最优）；「温度采样」：通过温度参数（0.2 和 0.7）控制采样随机性（温度低则生成结构更接近模板，温度高则更多样），每个靶点对生成 5000 个分子。筛选标准：从 5000 个生成分子中筛选 Top 候选，核心依据包括：「模型内在优先级」：采样频率（生成次数多的分子更受模型 “认可”）；「结构相似性」：与两个靶点模板集的几何平均相似度（确保同时关联两个靶点）；「分子对接」：用 AutoDock Vina 预测分子与两个靶点的结合自由能（ΔG），优先选择 ΔG 更负（结合亲和力更强）的分子。进行分子对接和结构对比确认。

文章2思路：
用深度学习生成模型（VAE）构建“可操控的化学空间”，再用强化学习（RL）在该空间中定向搜索同时满足多目标的分子，实现“从头设计”。
第一步：VAE将复杂的化学结构转化为低维的“嵌入向量”，把“分子生成”问题转化为“嵌入空间中的向量采样”。VAE只是先掌握通用分子生成能力。
VAE的组成：
1. 编码器（Encoder）：输入一个分子的 SMILES 字符串（如阿司匹林的 SMILES：CC (=O) OC1=CC=CC=C1C (=O) O），通过 GRU 网络将其压缩为 128 维的 “嵌入向量”（包含分子的拓扑结构、官能团、电子分布等核心信息）。
【嵌入空间满足 “结构相似性原则”—— 即结构相似的分子（如同一靶点的不同抑制剂）在嵌入空间中的向量距离更近，这为后续 “定向搜索” 提供了基础】
2. 解码器（Decoder）：输入一个 128 维的嵌入向量，通过另一个 GRU 网络反向生成符合化学规则的SMILES 字符串。
第二步：强化学习定向优化。定义6个奖励项，200轮迭代（采样-评分-筛选-更新VAE）。强化学习和VAE迭代交互。“边更新VAE，边生成优化分子”
第三步：对200轮迭代后生成的TOP100高奖励分子，实验验证（分子对接+体外实验）
