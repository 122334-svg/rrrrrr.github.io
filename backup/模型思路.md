数据预处理得到一个大的列表，一个列表中有多个data元素，一个data元素中有多个元素值（每个元素值一般都为一个张量）。前面数据预处理的输出，在loader中通过collate_fn将样本padding+stack到统一长度并堆叠（同时添加统一batch_size成第一个维度），统一长度指的是指同一batch_size的样本，如药物配体是取N_max，蛋白质口袋指M_max，这里的padding采用的是掩码技术，而非直接补0。然后将一个data中的特征张量分类拆成两个字典用于训练，drug = {'x','coors','num_atoms'}，protein = {'xyz','features'}，这里为什么边的特征被丢掉了？而且前面数据预处理每个小data中好像也没有边的特征。然后这个drug字典和protein字典同时输入模型内不同小模型进行训练，在模型中蛋白质口袋端通过pointnet++得到（batch_size，256），药物端通过EGNN提取特征也得到 (batch_size,256)，二者拼成 (batch_size,512)，采用直接拼接。做回归出一个标量（预测亲和力）。

上面关于边的问题已经得到解决，目前这个边的信息已经添加入模型。具体流程如下，在原始数据预处理的时候解析SDF文件，为每个原子提取了4维基本信息【原子类型索引、形式电荷、度数、芳香性】，同时构建了有向双边和3维的one-hot键类型edge_attr（SINGLE/DOUBLE/TRIPLE）。在训练collate_fn中，模型未将egde_attr直接喂给EGNN，而是统计每个原子连了多少条“单/双/三”键，得到一个【N,3】的计数向量，再与原子特征的4维基本向量【N,4】拼接成【N，7】，即最终的node_dim=7，这样等价于将“键信息”折叠进入节点特征通道里，从而我们会将EGNN的edge_dim设成0。

mask在EGNN里，是按批次、按原子位置的布尔张量，形状（batch_size，N）（N是本批次pad后的最大原子数）。下面演示下这个mask张量是如何生成的。
首先超参数：
batch_size=2 2个样本
max_num_stoms=6 统一填充到6个原子位置
ar=torch.arange(6)，tensor([0,1,2,3,4,5])，形状（6，）
num_atoms=tensor([3,5]) 样本0有3个原子，样本1有5个原子
num_atoms.unsqueeze(1) 将num_atoms添加维度，变成tensor([3],[5]) 形状（2，1）
广播后：at扩展为：
[[0,1,2,3,4,5],  # 样本0的索引序列
 [0,1,2,3,4,5]]  # 样本1的索引序列
广播后，num_atoms.unsqueeze(1) 扩展为：
[[3,3,3,3,3,3],  # 样本0的原子数（3）复制到6个位置
 [5,5,5,5,5,5]]  # 样本1的原子数（5）复制到6个位置
比较结果mask(形状（2，6）)：
[[True, True, True, False, False, False],  # 样本0：0-2是有效原子（<3），3-5是填充
 [True, True, True, True, True, False]]   # 样本1：0-4是有效原子（<5），5是填充
本质就是compare，用列表的元素个数去对比ar，就可以知道到底哪些是有效位，哪些是padding后才产生的元素。
在模型中mask的作用：传给EGNN，让它在消息传递时屏蔽padding的节点/边。池化（mean/sum）读出时，只按True的位置做归一化，不把padding算进去。

对于pointnet2和egnn的具体模型代码、使用、超参数设置等内容可见github源码。



