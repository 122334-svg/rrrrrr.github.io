数据预处理得到一个大的列表，一个列表中有多个data元素，一个data元素中有多个元素值（每个元素值一般都为一个张量）。前面数据预处理的输出，在loader中通过collate_fn将样本padding+stack到统一长度并堆叠（同时添加统一batch_size成第一个维度），统一长度指的是指同一batch_size的样本，如药物配体是取N_max，蛋白质口袋指M_max，这里的padding采用的是掩码技术，而非直接补0。然后将一个data中的特征张量分类拆成两个字典用于训练，drug = {'x','coors','num_atoms'}，protein = {'xyz','features'}，这里为什么边的特征被丢掉了？而且前面数据预处理每个小data中好像也没有边的特征。然后这个drug字典和protein字典同时输入模型内不同小模型进行训练，在模型中蛋白质口袋端通过pointnet++得到（batch_size，256），药物端通过EGNN提取特征也得到 (batch_size,256)，二者拼成 (batch_size,512)，采用直接拼接。做回归出一个标量（预测亲和力）。

上面关于边的问题已经得到解决，目前这个边的信息已经添加入模型。具体流程如下，在原始数据预处理的时候解析SDF文件，为每个原子提取了4维基本信息【原子类型索引、形式电荷、度数、芳香性】，同时构建了有向双边和3维的one-hot键类型edge_attr（SINGLE/DOUBLE/TRIPLE）。在训练collate_fn中，模型未将egde_attr直接喂给EGNN，而是统计每个原子连了多少条“单/双/三”键，得到一个【N,3】的计数向量，再与原子特征的4维基本向量【N,4】拼接成【N，7】，即最终的node_dim=7，这样等价于将“键信息”折叠进入节点特征通道里，从而我们会将EGNN的edge_dim设成0。

mask在EGNN里，是按批次、按原子位置的布尔张量，形状（batch_size，N）（N是本批次pad后的最大原子数）。下面演示下这个mask张量是如何生成的。
首先超参数：
batch_size=2 2个样本
max_num_stoms=6 统一填充到6个原子位置
ar=torch.arange(6)，tensor([0,1,2,3,4,5])，形状（6，）
num_atoms=tensor([3,5]) 样本0有3个原子，样本1有5个原子
num_atoms.unsqueeze(1) 将num_atoms添加维度，变成tensor([3],[5]) 形状（2，1）
广播后：at扩展为：
[[0,1,2,3,4,5],  # 样本0的索引序列
 [0,1,2,3,4,5]]  # 样本1的索引序列
广播后，num_atoms.unsqueeze(1) 扩展为：
[[3,3,3,3,3,3],  # 样本0的原子数（3）复制到6个位置
 [5,5,5,5,5,5]]  # 样本1的原子数（5）复制到6个位置
比较结果mask(形状（2，6）)：
[[True, True, True, False, False, False],  # 样本0：0-2是有效原子（<3），3-5是填充
 [True, True, True, True, True, False]]   # 样本1：0-4是有效原子（<5），5是填充
本质就是compare，用列表的元素个数去对比ar，就可以知道到底哪些是有效位，哪些是padding后才产生的元素。
在模型中mask的作用：传给EGNN，让它在消息传递时屏蔽padding的节点/边。池化（mean/sum）读出时，只按True的位置做归一化，不把padding算进去。（一般有padding就会有mask）

在处理EGNN网络时，要将分子“数字化”，我们需要三种核心信息，第一个是原子信息（离散特征：原子类型（用数字表示，比如 C=1，O=2，N=3）、连续特征：原子的物理化学属性：带多少电荷、形成几个化学键等（用具体数据表示）），第二个是原子坐标，第三个是化学键信息。

这个EGNN的思路就是：首先输入数据预处理数据，把原始分子数据（原子特征、坐标、化学键）嵌入，将电荷、成键数这些连续数值，用一个简单的线性层转换为和嵌入向量同长度的向量，将原子种类嵌入向量和连续特征向量拼接在一起，再通过一个线性层统一成固定长度的向量。第二部，处理化学键，原始的稀疏化学键变成稠密化学键，将键类型（独热编码，三维）+距离（1维）拼接，得到4维特征矩阵。第三步，padding+mask。第四步，消息传递，等变更新特征。第五步，输出分子全局特征，将每个原子的特征汇总成整个分子的特征（用于预测性质））

这里解释下关于嵌入向量的问题（见上面EGNN的原始分子数据）：为什么要用嵌入层，如果简单用1表示C原子，2表示O原子，3表示N原子，这样会带来虚假的大小关系+错误的相似性（1和2距离近他们是不是相似度高），而embdding层通过将每个原子种类（C O）转换成一个向量比如 C=[0.2, 0.5, -0.1]，O=[-0.3, 0.4, 0.2]，N=[-0.2, 0.3, 0.1]，这样能消除虚假的大小关系，跟能扑捉真实的相似性，向量的每一个维度能带白哦原子的一种潜在属性（比如 “是否容易得电子”“成键能力强弱”“原子半径大小” 等），模型训练的时候会自动调节这些数值，让向量更贴合原子的真实性质。（这个向量中的值一开始是随机指定的，后面是在训练中不断优化的）

对于pointnet2和egnn的具体模型代码、使用、超参数设置等内容可见github源码。



